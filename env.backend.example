# Arona Backend Configuration (legacy format)
# Copy this file to `.env` (recommended) or `.env.backend` (legacy) and customize.

# ============================================================================
# LLM Configuration (Required)
# ============================================================================
LLM_PROVIDER=openai                    # openai, azure, anthropic, custom, local
LLM_MODEL_NAME=gpt-4o-mini             # Model identifier
LLM_API_KEY=your-api-key-here          # API key for the provider
LLM_BASE_URL=https://api.openai.com/v1 # Optional: Custom base URL
LLM_TEMPERATURE=0.7                    # Optional: Sampling temperature
LLM_MAX_TOKENS=4096                    # Optional: Max tokens in response

# ============================================================================
# Embedding Configuration (Required)
# ============================================================================
EMBEDDING_PROVIDER=openai              # openai, azure, custom, local (OpenAI-compatible HTTP), local_gpu (in-process GPU)
EMBEDDING_MODEL_NAME=text-embedding-3-large
EMBEDDING_API_KEY=your-api-key-here
EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_EMBEDDING_DIM=3072           # Required: Embedding dimension

# ============================================================================
# Vision Model Configuration (Optional)
# ============================================================================
VISION_PROVIDER=openai                 # openai, azure, custom, local
VISION_MODEL_NAME=gpt-4o               # Vision-capable model
VISION_API_KEY=your-api-key-here
VISION_BASE_URL=https://api.openai.com/v1

# ============================================================================
# Reranker Configuration (Optional)
# ============================================================================
RERANKER_ENABLED=true                  # Enable/disable reranking
RERANKER_PROVIDER=local                # local or api
RERANKER_MODEL_PATH=~/.huggingface/models/bge-reranker-v2-gemma
RERANKER_DTYPE=float16                 # Reranker dtype: float16 | float32
RERANKER_BATCH_SIZE=16                 # Batch size for reranking

# ============================================================================
# Storage Configuration
# ============================================================================
WORKING_DIR=./rag_storage              # RAG storage directory
UPLOAD_DIR=./uploads                   # Upload directory for documents

# ============================================================================
# RAGAnything Configuration
# ============================================================================
PARSER=mineru                          # Parser: mineru or docling
ENABLE_IMAGE_PROCESSING=true           # Process images in documents
ENABLE_TABLE_PROCESSING=true           # Process tables in documents
ENABLE_EQUATION_PROCESSING=true        # Process equations in documents

# ============================================================================
# API Server Configuration
# ============================================================================
API_HOST=0.0.0.0                       # Host to bind to
API_PORT=8000                          # Port to bind to
CORS_ORIGINS=*                         # CORS allowed origins (comma-separated)

# =============================================================================
# CUDA Runtime Path (Local GPU, Pascal/cu118)
# =============================================================================
# Note: The start script also exports this path before launching Python so the
# dynamic loader sees the correct cuBLAS/CUDA libs from the venv.
LD_LIBRARY_PATH=.venv/lib/python3.11/site-packages/torch/lib

# ============================================================================
# Example Configurations
# ============================================================================

# --- OpenAI Configuration ---
# LLM_PROVIDER=openai
# LLM_MODEL_NAME=gpt-4o-mini
# LLM_API_KEY=sk-...
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL_NAME=text-embedding-3-large
# EMBEDDING_API_KEY=sk-...
# EMBEDDING_EMBEDDING_DIM=3072

# --- Azure OpenAI Configuration ---
# LLM_PROVIDER=azure
# LLM_MODEL_NAME=gpt-4o-mini
# LLM_API_KEY=your-azure-key
# LLM_BASE_URL=https://your-resource.openai.azure.com/
# EMBEDDING_PROVIDER=azure
# EMBEDDING_MODEL_NAME=text-embedding-3-large
# EMBEDDING_API_KEY=your-azure-key
# EMBEDDING_BASE_URL=https://your-resource.openai.azure.com/
# EMBEDDING_EMBEDDING_DIM=3072

# --- LM Studio (Local) Configuration ---
# LLM_PROVIDER=local
# LLM_MODEL_NAME=local-model
# LLM_API_KEY=not-needed
# LLM_BASE_URL=http://localhost:1234/v1
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL_NAME=nomic-embed-text
# EMBEDDING_API_KEY=not-needed
# EMBEDDING_BASE_URL=http://localhost:1234/v1
# EMBEDDING_EMBEDDING_DIM=768
# RERANKER_ENABLED=false

# --- Custom API (vLLM, TGI, etc.) Configuration ---
# LLM_PROVIDER=custom
# LLM_MODEL_NAME=Qwen/Qwen2.5-72B-Instruct
# LLM_API_KEY=your-custom-key
# LLM_BASE_URL=https://your-vllm-endpoint.com/v1
# EMBEDDING_PROVIDER=custom
# EMBEDDING_MODEL_NAME=BAAI/bge-large-en-v1.5
# EMBEDDING_API_KEY=your-custom-key
# EMBEDDING_BASE_URL=https://your-embedding-endpoint.com/v1
# EMBEDDING_EMBEDDING_DIM=1024

# ============================================================================
# Local GPU Embedding Configuration (Plan B - 3x GTX 1080 Ti)
# ============================================================================
# Use local GPU-based embedding and reranking models instead of online APIs.
# This configuration is optimized for 3x NVIDIA GTX 1080 Ti (Pascal, 11GB VRAM each).
#
# Prerequisites:
# 1. Install PyTorch with CUDA 11.8:
#    pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118
# 2. Install transformers and dependencies:
#    pip install transformers==4.51.3 sentence-transformers>=2.3.0 accelerate safetensors pillow
# 3. Verify Pascal compatibility:
#    python scripts/verify_pascal.py
# 4. Download models (will be cached in HF_HOME):
#    python scripts/download_local_models.py --model Qwen/Qwen3-Embedding-4B --device cuda:0
#    python scripts/download_local_models.py --model Qwen/Qwen3-Reranker-4B --device cuda:1
#
# GPU Allocation Strategy:
# - GPU 0 (cuda:0): Qwen3-Embedding-4B (text embedding, 2560-dim)
# - GPU 1 (cuda:1): Qwen3-Reranker-4B (reranking)
# - GPU 2 (cuda:2): GME-Qwen2-VL-2B (multimodal, optional)

# --- Text Embedding (GPU 0) ---
# EMBEDDING_PROVIDER=local_gpu
# EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-4B
# EMBEDDING_EMBEDDING_DIM=2560
# EMBEDDING_DEVICE=cuda:0
# EMBEDDING_DTYPE=float16
# EMBEDDING_ATTN_IMPLEMENTATION=sdpa  # or 'eager' for maximum compatibility
#
# Performance Tuning Parameters:
# EMBEDDING_MAX_BATCH_SIZE=32          # Maximum number of concurrent requests to batch (default: 32)
#                                      # Increase to 64-128 for higher concurrency scenarios
#                                      # Recommended: 32-64 for 11GB VRAM
# EMBEDDING_MAX_WAIT_TIME=0.1          # Maximum wait time in seconds for batch collection (default: 0.1)
#                                      # Lower values reduce latency, higher values increase throughput
#                                      # Recommended: 0.05-0.2 for balanced performance
#                                      # Alias: EMBEDDING_MAX_QUEUE_TIME (deprecated) still supported
# EMBEDDING_MAX_BATCH_TOKENS=16384     # Maximum tokens per batch (optional)
#                                      # Limits batch size based on total token count
#                                      # Recommended: 8192-16384 for memory efficiency
# EMBEDDING_ENCODE_BATCH_SIZE=128      # Internal batch size for sentence-transformers (default: 128)
#                                      # Controls parallel processing during inference
#                                      # Higher values improve GPU utilization but increase VRAM
#
# Measured Performance (GTX 1080 Ti, FP16, Qwen3-Embedding-4B):
# - Throughput: ~28 texts/sec (concurrent requests, model-limited)
# - Latency: p95 = 192.58ms (single request)
# - Memory: Peak 8.34GB VRAM (batch size 128)
#
# Performance Notes:
# - GTX 1080 Ti (Pascal) has limited throughput due to older architecture
# - For >100 texts/sec, consider: RTX 3090/4090, A100, or smaller models
# - BatchProcessor efficiently merges concurrent requests for optimal GPU usage
#
# Benchmark: python scripts/benchmark_local_embedding.py --mode all --device cuda:0

# --- Reranker (GPU 1) ---
# RERANKER_ENABLED=true
# RERANKER_PROVIDER=local_gpu  # alias: "local" also supported (device=cuda:* selects GPU path)
# RERANKER_MODEL_NAME=Qwen/Qwen3-Reranker-4B
# RERANKER_DEVICE=cuda:1
# RERANKER_DTYPE=float16
# RERANKER_ATTN_IMPLEMENTATION=sdpa    # or 'eager' for maximum compatibility
# RERANKER_MAX_LENGTH=8192             # Qwen3-Reranker supports 8k context
# RERANKER_BATCH_SIZE=16               # Reranker typically uses smaller batches
#                                      # Recommended: 8-32 for 11GB VRAM
#
# Expected Performance (GTX 1080 Ti, FP16):
# - Memory: < 11GB VRAM (peak usage)
# - Batch processing: 8-32 query-document pairs

# --- Multimodal Embedding (GPU 1, Optional) ---
# MULTIMODAL_EMBEDDING_ENABLED=false  # Set to true to enable
# MULTIMODAL_EMBEDDING_PROVIDER=local_gpu
# MULTIMODAL_EMBEDDING_MODEL_NAME=Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
# MULTIMODAL_EMBEDDING_EMBEDDING_DIM=1536
# MULTIMODAL_EMBEDDING_DEVICE=cuda:2  # Recommended if you have a 3rd GPU; otherwise use an available cuda device
# MULTIMODAL_EMBEDDING_DTYPE=float16
# MULTIMODAL_EMBEDDING_ATTN_IMPLEMENTATION=sdpa
# MULTIMODAL_EMBEDDING_MIN_IMAGE_TOKENS=256
# MULTIMODAL_EMBEDDING_MAX_IMAGE_TOKENS=1280
# MULTIMODAL_EMBEDDING_MAX_LENGTH=1800
# MULTIMODAL_EMBEDDING_ALLOW_IMAGE_URLS=false  # Security: keep false in production unless you trust the network
# MULTIMODAL_EMBEDDING_MAX_IMAGE_BYTES=10485760 # 10MB limit when ALLOW_IMAGE_URLS=true
#
# Expected Performance (GTX 1080 Ti, FP16):
# - Memory: 4.12 GB baseline, 8.54 GB peak (batch_size=8)
# - Latency: ~0.029s/text (text-only), ~0.691s/pair (text+image)
# - Throughput: 2.05 pairs/s (batch_size=1), 1.46 pairs/s (batch_size=4), 0.91 pairs/s (batch_size=8)
# - Max batch size: 8 (batch_size=16 causes OOM on 11GB GPU)
# - Supports text + image joint embedding

# --- HuggingFace Configuration (Required for local models) ---
# Set HF_HOME in your shell environment (not in .env file):
# export HF_HOME=/path/to/huggingface_cache
# export HF_HUB_CACHE=$HF_HOME/hub
#
# For China/restricted networks, use ModelScope mirror:
# export HF_ENDPOINT=https://hf-mirror.com
# export MINERU_MODEL_SOURCE=modelscope

# ============================================================================
# Local GPU Embedding/Reranking (2x GTX 1080 Ti) - Qwen3-VL (2B)
# ============================================================================
# If you only have 2 GPUs (e.g. 2x 1080 Ti), the simplest allocation is:
# - GPU 0 (cuda:0): Qwen3-VL-Embedding-2B
# - GPU 1 (cuda:1): Qwen3-VL-Reranker-2B
#
# Notes:
# - Qwen3-VL models require transformers>=4.57.0 (Qwen3-VL support). Upgrading transformers may
#   break the legacy GME-Qwen2-VL provider pinned to transformers==4.51.3 in this repo.
# - The current Arona retrieval pipeline passes *text only* into embedding/reranking. Qwen3-VL
#   will run in a text-only mode unless you extend the pipeline to pass images/videos.
# - For Pascal (GTX 1080 Ti), avoid flash_attention_2. Use attn_implementation=sdpa or eager.
#
# --- Text Embedding (GPU 0) ---
# EMBEDDING_PROVIDER=local_gpu
# EMBEDDING_MODEL_NAME=Qwen/Qwen3-VL-Embedding-2B
# EMBEDDING_DEVICE=cuda:0
# EMBEDDING_DTYPE=float16
# EMBEDDING_ATTN_IMPLEMENTATION=sdpa
# EMBEDDING_MAX_LENGTH=8192
# EMBEDDING_EMBEDDING_DIM=2048         # Can set smaller (e.g. 1024) to reduce VRAM / speed up ANN
# EMBEDDING_DEFAULT_INSTRUCTION=Represent the user's input.
# EMBEDDING_MIN_IMAGE_TOKENS=4          # Qwen3-VL: image token lower bound (default: 4)
# EMBEDDING_MAX_IMAGE_TOKENS=1280       # Qwen3-VL: lower this if you see OOM (default: 1280)
#
# --- Reranker (GPU 1) ---
# RERANKER_ENABLED=true
# RERANKER_PROVIDER=local_gpu  # alias: "local" also supported (device=cuda:* selects GPU path)
# RERANKER_MODEL_NAME=Qwen/Qwen3-VL-Reranker-2B
# RERANKER_DEVICE=cuda:1
# RERANKER_DTYPE=float16
# RERANKER_ATTN_IMPLEMENTATION=sdpa
# RERANKER_MAX_LENGTH=8192
# RERANKER_BATCH_SIZE=8
# RERANKER_MIN_IMAGE_TOKENS=4           # Qwen3-VL: image token lower bound (default: 4)
# RERANKER_MAX_IMAGE_TOKENS=1280        # Qwen3-VL: lower this if you see OOM (default: 1280)
#
# Optional instruction control (instruction-aware reranking):
# RERANKER_INSTRUCTION=Given a search query, retrieve relevant candidates that answer the query.
# RERANKER_SYSTEM_PROMPT=Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".
