# Arona Backend Configuration
# Copy this file to .env.backend and customize for your deployment

# ============================================================================
# LLM Configuration (Required)
# ============================================================================
LLM_PROVIDER=openai                    # openai, azure, anthropic, custom, local
LLM_MODEL_NAME=gpt-4o-mini             # Model identifier
LLM_API_KEY=your-api-key-here          # API key for the provider
LLM_BASE_URL=https://api.openai.com/v1 # Optional: Custom base URL
LLM_TEMPERATURE=0.7                    # Optional: Sampling temperature
LLM_MAX_TOKENS=4096                    # Optional: Max tokens in response

# ============================================================================
# Embedding Configuration (Required)
# ============================================================================
EMBEDDING_PROVIDER=openai              # openai, azure, custom, local
EMBEDDING_MODEL_NAME=text-embedding-3-large
EMBEDDING_API_KEY=your-api-key-here
EMBEDDING_BASE_URL=https://api.openai.com/v1
EMBEDDING_EMBEDDING_DIM=3072           # Required: Embedding dimension

# ============================================================================
# Vision Model Configuration (Optional)
# ============================================================================
VISION_PROVIDER=openai                 # openai, azure, custom, local
VISION_MODEL_NAME=gpt-4o               # Vision-capable model
VISION_API_KEY=your-api-key-here
VISION_BASE_URL=https://api.openai.com/v1

# ============================================================================
# Reranker Configuration (Optional)
# ============================================================================
RERANKER_ENABLED=true                  # Enable/disable reranking
RERANKER_PROVIDER=local                # local or api
RERANKER_MODEL_PATH=~/.huggingface/models/bge-reranker-v2-gemma
RERANKER_USE_FP16=false                # Use FP16 for local reranker
RERANKER_BATCH_SIZE=16                 # Batch size for reranking

# ============================================================================
# Storage Configuration
# ============================================================================
WORKING_DIR=./rag_storage              # RAG storage directory
UPLOAD_DIR=./uploads                   # Upload directory for documents

# ============================================================================
# RAGAnything Configuration
# ============================================================================
PARSER=mineru                          # Parser: mineru or docling
ENABLE_IMAGE_PROCESSING=true           # Process images in documents
ENABLE_TABLE_PROCESSING=true           # Process tables in documents
ENABLE_EQUATION_PROCESSING=true        # Process equations in documents

# ============================================================================
# API Server Configuration
# ============================================================================
API_HOST=0.0.0.0                       # Host to bind to
API_PORT=8000                          # Port to bind to
CORS_ORIGINS=*                         # CORS allowed origins (comma-separated)

# =============================================================================
# CUDA Runtime Path (Local GPU, Pascal/cu118)
# =============================================================================
# Note: The start script also exports this path before launching Python so the
# dynamic loader sees the correct cuBLAS/CUDA libs from the venv.
LD_LIBRARY_PATH=.venv/lib/python3.11/site-packages/torch/lib

# ============================================================================
# Example Configurations
# ============================================================================

# --- OpenAI Configuration ---
# LLM_PROVIDER=openai
# LLM_MODEL_NAME=gpt-4o-mini
# LLM_API_KEY=sk-...
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL_NAME=text-embedding-3-large
# EMBEDDING_API_KEY=sk-...
# EMBEDDING_EMBEDDING_DIM=3072

# --- Azure OpenAI Configuration ---
# LLM_PROVIDER=azure
# LLM_MODEL_NAME=gpt-4o-mini
# LLM_API_KEY=your-azure-key
# LLM_BASE_URL=https://your-resource.openai.azure.com/
# EMBEDDING_PROVIDER=azure
# EMBEDDING_MODEL_NAME=text-embedding-3-large
# EMBEDDING_API_KEY=your-azure-key
# EMBEDDING_BASE_URL=https://your-resource.openai.azure.com/
# EMBEDDING_EMBEDDING_DIM=3072

# --- LM Studio (Local) Configuration ---
# LLM_PROVIDER=local
# LLM_MODEL_NAME=local-model
# LLM_API_KEY=not-needed
# LLM_BASE_URL=http://localhost:1234/v1
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL_NAME=nomic-embed-text
# EMBEDDING_API_KEY=not-needed
# EMBEDDING_BASE_URL=http://localhost:1234/v1
# EMBEDDING_EMBEDDING_DIM=768
# RERANKER_ENABLED=false

# --- Custom API (vLLM, TGI, etc.) Configuration ---
# LLM_PROVIDER=custom
# LLM_MODEL_NAME=Qwen/Qwen2.5-72B-Instruct
# LLM_API_KEY=your-custom-key
# LLM_BASE_URL=https://your-vllm-endpoint.com/v1
# EMBEDDING_PROVIDER=custom
# EMBEDDING_MODEL_NAME=BAAI/bge-large-en-v1.5
# EMBEDDING_API_KEY=your-custom-key
# EMBEDDING_BASE_URL=https://your-embedding-endpoint.com/v1
# EMBEDDING_EMBEDDING_DIM=1024

# ============================================================================
# Local GPU Embedding Configuration (Plan B - 3x GTX 1080 Ti)
# ============================================================================
# Use local GPU-based embedding and reranking models instead of online APIs.
# This configuration is optimized for 3x NVIDIA GTX 1080 Ti (Pascal, 11GB VRAM each).
#
# Prerequisites:
# 1. Install PyTorch with CUDA 11.8:
#    pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118
# 2. Install transformers and dependencies:
#    pip install transformers==4.51.3 sentence-transformers>=2.3.0 accelerate safetensors pillow
# 3. Verify Pascal compatibility:
#    python scripts/verify_pascal.py
# 4. Download models (will be cached in HF_HOME):
#    python scripts/download_local_models.py --model Qwen/Qwen3-Embedding-4B --device cuda:0
#    python scripts/download_local_models.py --model Qwen/Qwen3-Reranker-4B --device cuda:1
#
# GPU Allocation Strategy:
# - GPU 0 (cuda:0): Qwen3-Embedding-4B (text embedding, 2560-dim)
# - GPU 1 (cuda:1): Qwen3-Reranker-4B (reranking)
# - GPU 2 (cuda:2): GME-Qwen2-VL-2B (multimodal, optional)

# --- Text Embedding (GPU 0) ---
# EMBEDDING_PROVIDER=local_gpu
# EMBEDDING_MODEL_NAME=Qwen/Qwen3-Embedding-4B
# EMBEDDING_EMBEDDING_DIM=2560
# EMBEDDING_DEVICE=cuda:0
# EMBEDDING_DTYPE=float16
# EMBEDDING_ATTN_IMPLEMENTATION=sdpa  # or 'eager' for maximum compatibility
# EMBEDDING_BATCH_SIZE=32  # Adjust based on available VRAM
# EMBEDDING_MAX_SEQ_LENGTH=512  # Maximum sequence length for embedding

# --- Reranker (GPU 1) ---
# RERANKER_ENABLED=true
# RERANKER_PROVIDER=local_gpu
# RERANKER_MODEL_NAME=Qwen/Qwen3-Reranker-4B
# RERANKER_DEVICE=cuda:1
# RERANKER_DTYPE=float16
# RERANKER_BATCH_SIZE=16  # Reranker typically uses smaller batches
# RERANKER_MAX_SEQ_LENGTH=512

# --- Multimodal Embedding (GPU 2, Optional) ---
# MULTIMODAL_EMBEDDING_ENABLED=false  # Set to true to enable
# MULTIMODAL_EMBEDDING_PROVIDER=local_gpu
# MULTIMODAL_EMBEDDING_MODEL_NAME=Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
# MULTIMODAL_EMBEDDING_EMBEDDING_DIM=1536
# MULTIMODAL_EMBEDDING_DEVICE=cuda:2
# MULTIMODAL_EMBEDDING_DTYPE=float16
# MULTIMODAL_EMBEDDING_ATTN_IMPLEMENTATION=sdpa

# --- HuggingFace Configuration (Required for local models) ---
# Set HF_HOME in your shell environment (not in .env file):
# export HF_HOME=/path/to/huggingface_cache
# export HF_HUB_CACHE=$HF_HOME/hub
#
# For China/restricted networks, use ModelScope mirror:
# export HF_ENDPOINT=https://hf-mirror.com
# export MINERU_MODEL_SOURCE=modelscope
