# Backend API Dependencies

# FastAPI and server
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6  # For file uploads

# Data validation and configuration
pydantic>=2.0.0
pydantic-settings>=2.0.0
pyyaml>=6.0

# HTTP client (for testing)
httpx>=0.25.0
requests>=2.31.0

# Async support
aiofiles>=23.0.0

# Logging and monitoring
python-json-logger>=2.0.0

# ============================================================================
# Local Embedding Dependencies (Optional - for Plan B local GPU deployment)
# ============================================================================
# NOTE: These dependencies are ONLY needed if you plan to use local GPU-based
# embedding/reranking (EMBEDDING_PROVIDER=local_gpu). If using online APIs
# (OpenAI, Jina, etc.), you can skip installing these packages.
#
# Installation instructions:
# 1. Install PyTorch with CUDA 11.8 support (for Pascal GTX 1080 Ti):
#    pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.13.1 --index-url https://download.pytorch.org/whl/cu118
#
# 2. Install transformers and related packages:
#    pip install transformers==4.57.1 sentence-transformers>=2.3.0 accelerate safetensors pillow
#
# Version constraints explained:
# - torch==2.0.1: Last stable version with full Pascal (GTX 1080 Ti) support via CUDA 11.8
# - transformers==4.57.1: REQUIRED for GME-Qwen2-VL-2B compatibility. DO NOT upgrade to 4.52+
#   as it breaks GME model loading. See: https://github.com/Alibaba-NLP/GME/issues/xxx
# - sentence-transformers>=2.3.0: Stable version with good SentenceTransformer API
# - accelerate>=0.25.0: For efficient model loading and device mapping
# - safetensors>=0.4.0: Fast and safe model weight loading
# - pillow>=10.0.0: Required for multimodal (image) embedding support

# PyTorch (CUDA 11.8 for Pascal architecture)
# Install separately: pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cu118
# torch==2.0.1

# Transformers and NLP
transformers==4.57.1  # LOCKED: Required for GME compatibility, DO NOT upgrade to 4.52+
sentence-transformers>=2.3.0  # For embedding model inference
accelerate>=0.25.0  # Model loading optimization
safetensors>=0.4.0  # Fast model weight loading

# NumPy (compatibility constraint)
numpy<2  # LOCKED: sentence-transformers has compatibility issues with NumPy 2.x

# Image processing (for multimodal embedding)
pillow>=10.0.0  # Already in raganything optional deps, but needed for GME

