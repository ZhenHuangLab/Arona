"""
Qwen3-VL local GPU providers (Embedding + Reranker).

These providers are intended to "fit into" the existing Arona backend model
factory and LightRAG/RAGAnything contracts:

- Embedding provider: accepts ``List[str]`` and returns ``np.ndarray`` (float32)
- Reranker provider: accepts ``query: str`` + ``documents: List[str]`` and returns
  ``List[float]`` relevance scores

Important limitations (current codebase):
- LightRAG's vector/rerank contracts are text-based. We enable multimodal behavior by
  parsing ``Image Path: ...`` markers from chunk templates (generated by RAGAnything)
  and loading the corresponding local image files.
"""

from __future__ import annotations

import asyncio
import logging
import os
import re
from typing import Any, Dict, List, Optional

import numpy as np

from backend.config import ModelConfig
from backend.providers.base import BaseEmbeddingProvider, BaseRerankerProvider

logger = logging.getLogger(__name__)

_IMAGE_PATH_RE = re.compile(
    r"(?im)^[ \t]*Image Path:[ \t]*(?P<path>.+?)[ \t]*$", re.MULTILINE
)

# Qwen3-VL uses "image tokens" which are derived from pixel count. The official
# scripts use IMAGE_BASE_FACTOR=16 and image_patch_size=16; we keep the same.
_IMAGE_BASE_FACTOR = 16
_IMAGE_FACTOR = _IMAGE_BASE_FACTOR * 2  # 32
_PIXELS_PER_IMAGE_TOKEN = _IMAGE_FACTOR * _IMAGE_FACTOR  # 1024


def _extract_image_path(content: str) -> Optional[str]:
    """Extract the first 'Image Path: ...' from a chunk template."""
    match = _IMAGE_PATH_RE.search(content or "")
    if not match:
        return None
    path = match.group("path").strip().strip('"').strip("'")
    return path or None


def _strip_image_path_line(content: str) -> str:
    """Remove 'Image Path: ...' line from text to avoid path leakage into embeddings."""
    return _IMAGE_PATH_RE.sub("", content or "").strip()


def _maybe_prefix_file_scheme(path: str) -> str:
    """Ensure local paths are passed as file:// URIs (what qwen-vl-utils expects)."""
    path = path.strip()
    if path.startswith(("http://", "https://", "oss://", "file://")):
        return path
    return "file://" + path


def _image_tokens_to_pixels(tokens: int) -> int:
    return int(tokens) * _PIXELS_PER_IMAGE_TOKEN


def _resolve_torch_dtype(dtype_str: str, *, torch_module: Any, device: str) -> Any:
    """Resolve dtype string to a torch dtype with sensible fallbacks."""

    dtype_normalized = (dtype_str or "").lower().strip()

    if dtype_normalized in {"fp16", "float16", "torch.float16"}:
        return torch_module.float16
    if dtype_normalized in {"fp32", "float32", "torch.float32"}:
        return torch_module.float32
    if dtype_normalized in {"bf16", "bfloat16", "torch.bfloat16"}:
        # GTX 1080 Ti (sm_61) does not support bfloat16 efficiently; keep it explicit
        return getattr(torch_module, "bfloat16", torch_module.float16)

    # Default: float16 on CUDA, float32 on CPU.
    if isinstance(device, str) and device.startswith("cuda"):
        return torch_module.float16
    return torch_module.float32


def _pool_last_token(
    hidden_state: Any, attention_mask: Any, *, torch_module: Any
) -> Any:
    """
    Pool embeddings by selecting the last non-padding token according to attention_mask.

    Works for both left- and right-padded sequences.
    """
    if attention_mask is None:
        return hidden_state[:, -1]

    flipped = attention_mask.flip(dims=[1])
    last_one_positions = flipped.argmax(dim=1)
    col = attention_mask.shape[1] - last_one_positions - 1
    row = torch_module.arange(hidden_state.shape[0], device=hidden_state.device)
    return hidden_state[row, col]


def _as_positive_int(value: Any) -> Optional[int]:
    """Convert value to a positive int if possible."""
    try:
        parsed = int(value)
    except (TypeError, ValueError):
        return None
    if parsed <= 0:
        return None
    return parsed


class Qwen3VLEmbeddingProvider(BaseEmbeddingProvider):
    """Local GPU embedding provider using Qwen3-VL-Embedding-* models."""

    def __init__(self, config: ModelConfig):
        self.config = config
        self.model_name = config.model_name

        self.device = config.extra_params.get("device", "cuda:0")
        dtype_str = config.extra_params.get("dtype", "float16")
        attn_impl = config.extra_params.get("attn_implementation", "sdpa")

        # Qwen3-VL supports up to 32k tokens, but for 1080Ti we strongly recommend
        # keeping this lower (e.g. 4k-8k) to avoid OOM.
        self.max_length = int(config.extra_params.get("max_length", 8192))
        self.min_image_tokens = int(config.extra_params.get("min_image_tokens", 4))
        self.max_image_tokens = int(config.extra_params.get("max_image_tokens", 1280))
        self.allow_image_urls = bool(config.extra_params.get("allow_image_urls", False))
        self.default_instruction = config.extra_params.get(
            "default_instruction", "Represent the user's input."
        )
        self.normalize = bool(config.extra_params.get("normalize", True))

        self._lock = asyncio.Lock()

        model_path = config.extra_params.get("model_path", self.model_name)

        try:
            import torch  # pylint: disable=import-error
            from qwen_vl_utils import process_vision_info  # pylint: disable=import-error
            from transformers import (  # pylint: disable=import-error
                AutoProcessor,
                Qwen3VLForConditionalGeneration,
            )
        except Exception as exc:  # pylint: disable=broad-except
            raise RuntimeError(
                "Qwen3-VL embedding provider requires a working torch + transformers installation. "
                "If you're on GTX 1080 Ti, prefer torch CUDA 11.8 wheels (e.g. torch==2.7.1+cu118) "
                "and transformers>=4.57.0 (Qwen3-VL support), plus qwen-vl-utils."
            ) from exc

        self._torch = torch
        self._process_vision_info = process_vision_info
        self.dtype = _resolve_torch_dtype(
            dtype_str, torch_module=torch, device=self.device
        )

        if self.min_image_tokens <= 0 or self.max_image_tokens <= 0:
            raise ValueError(
                f"Invalid image token limits: min_image_tokens={self.min_image_tokens}, "
                f"max_image_tokens={self.max_image_tokens}"
            )
        if self.min_image_tokens > self.max_image_tokens:
            raise ValueError(
                f"min_image_tokens ({self.min_image_tokens}) > max_image_tokens ({self.max_image_tokens})"
            )

        logger.info(
            "Loading Qwen3-VL embedding model: %s (path=%s, device=%s, dtype=%s, attn=%s)",
            self.model_name,
            model_path,
            self.device,
            dtype_str,
            attn_impl,
        )

        try:
            lm = Qwen3VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=self.dtype,
                attn_implementation=attn_impl,
                trust_remote_code=True,
            )
            lm = lm.to(self.device)
            lm.eval()

            # Keep only the backbone to save memory (drop lm_head).
            self.model = lm.model

            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
            )
            if getattr(self.processor, "tokenizer", None) is not None:
                # Embedding pooling uses attention_mask, so padding side is flexible.
                self.processor.tokenizer.padding_side = "right"

            requested_dim = config.embedding_dim
            if requested_dim is not None:
                requested_dim = int(requested_dim)
                if requested_dim <= 0:
                    raise ValueError(f"Invalid embedding_dim: {requested_dim}")

            # We infer the true hidden size from a warmup forward pass instead of relying
            # on config.hidden_size. Qwen3-VL configs commonly nest the language config
            # (e.g., text_config.hidden_size) and may not expose hidden_size directly.
            #
            # To infer base_dim reliably, run warmup with a very large slice so we get the
            # full pooled representation (no truncation).
            temp_dim = (
                _as_positive_int(config.extra_params.get("infer_base_dim_max"))
                or 131072
            )
            temp_dim = max(temp_dim, 131072)
            self._embedding_dim = temp_dim

            # Warmup to allocate kernels and validate the forward path.
            logger.info("Performing Qwen3-VL embedding warmup inference...")
            warmup = self._embed_sync(
                ["warmup text"], instruction=self.default_instruction
            )
            base_dim = (
                int(warmup.shape[1])
                if hasattr(warmup, "shape") and len(warmup.shape) >= 2
                else None
            )
            if base_dim is None or base_dim <= 0:
                raise RuntimeError(
                    "Failed to infer Qwen3-VL embedding hidden size from warmup inference output."
                )

            if requested_dim is None:
                requested_dim = base_dim
            elif requested_dim > base_dim:
                raise ValueError(
                    f"Configured embedding_dim ({requested_dim}) exceeds model hidden size ({base_dim})."
                )

            self._base_dim = int(base_dim)
            self._embedding_dim = int(requested_dim)

            if (
                torch.cuda.is_available()
                and isinstance(self.device, str)
                and self.device.startswith("cuda")
            ):
                allocated = torch.cuda.memory_allocated(self.device) / 1024**3
                logger.info(
                    "GPU memory allocated after embedding warmup: %.2f GB", allocated
                )

            logger.info(
                "Qwen3-VL embedding model ready (base_dim=%s, output_dim=%s)",
                self._base_dim,
                self._embedding_dim,
            )
        except Exception as exc:
            logger.error(
                "Failed to load Qwen3-VL embedding model: %s", exc, exc_info=True
            )
            raise RuntimeError(
                f"Failed to load Qwen3-VL embedding model: {exc}"
            ) from exc

    @property
    def embedding_dim(self) -> int:
        return self._embedding_dim

    async def embed(self, texts: List[str], **kwargs) -> np.ndarray:
        if not texts:
            return np.array([])

        instruction = kwargs.get("instruction", self.default_instruction)
        loop = asyncio.get_event_loop()

        async with self._lock:
            return await loop.run_in_executor(
                None, self._embed_sync, list(texts), instruction
            )

    def _embed_sync(self, texts: List[str], instruction: str) -> np.ndarray:
        torch = self._torch

        min_pixels = _image_tokens_to_pixels(self.min_image_tokens)
        max_pixels = _image_tokens_to_pixels(self.max_image_tokens)

        conversations: List[List[Dict[str, Any]]] = []
        for raw in texts:
            raw = raw or ""
            image_path = _extract_image_path(raw)
            has_local_image = False

            if image_path:
                if (
                    image_path.startswith(("http://", "https://"))
                    and not self.allow_image_urls
                ):
                    image_path = None
                elif image_path.startswith("file://"):
                    # file:// may be used in stored chunks; validate on filesystem.
                    has_local_image = os.path.exists(image_path[7:])
                else:
                    has_local_image = os.path.exists(image_path)
                    if not has_local_image and image_path.startswith("~"):
                        expanded = os.path.expanduser(image_path)
                        if expanded != image_path and os.path.exists(expanded):
                            image_path = expanded
                            has_local_image = True

            text_only = _strip_image_path_line(raw)
            if not text_only and not image_path:
                text_only = "NULL"

            user_content: List[Dict[str, Any]] = []
            if image_path and (
                has_local_image or image_path.startswith(("http://", "https://"))
            ):
                user_content.append(
                    {
                        "type": "image",
                        "image": _maybe_prefix_file_scheme(image_path),
                        "min_pixels": min_pixels,
                        "max_pixels": max_pixels,
                    }
                )
            if text_only:
                user_content.append({"type": "text", "text": text_only})

            conversations.append(
                [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": instruction}],
                    },
                    {"role": "user", "content": user_content},
                ]
            )

        prompt_text = self.processor.apply_chat_template(
            conversations,
            add_generation_prompt=True,
            tokenize=False,
        )

        with torch.no_grad():
            try:
                images, video_inputs, video_kwargs = self._process_vision_info(
                    conversations,
                    image_patch_size=_IMAGE_BASE_FACTOR,
                    return_video_metadata=True,
                    return_video_kwargs=True,
                )
            except Exception as exc:  # pylint: disable=broad-except
                # Fail-soft: if vision processing fails (bad image, missing deps), fall back to text-only
                logger.warning(
                    "process_vision_info failed; falling back to text-only embedding: %s",
                    exc,
                )
                images, video_inputs, video_kwargs = (
                    None,
                    None,
                    {"do_sample_frames": False},
                )

            if video_inputs is not None:
                videos, video_metadata = zip(*video_inputs)
                videos, video_metadata = list(videos), list(video_metadata)
            else:
                videos, video_metadata = None, None

            processed = self.processor(
                text=prompt_text,
                images=images,
                videos=videos,
                video_metadata=video_metadata,
                truncation=True,
                max_length=self.max_length,
                padding=True,
                do_resize=False,
                return_tensors="pt",
                **(video_kwargs or {}),
            )

            inputs = {
                k: v.to(self.device)
                for k, v in processed.items()
                if isinstance(v, torch.Tensor)
            }

            attention_mask = inputs.get("attention_mask")
            outputs = self.model(**inputs)
            hidden = outputs.last_hidden_state

            pooled = _pool_last_token(hidden, attention_mask, torch_module=torch)

            # MRL: allow users to request a smaller output dimension.
            pooled = pooled[:, : self._embedding_dim]

            if self.normalize:
                pooled = torch.nn.functional.normalize(pooled, p=2, dim=1)

            return pooled.float().cpu().numpy().astype(np.float32)

    async def shutdown(self) -> None:
        """Best-effort release of GPU/CPU resources."""
        async with self._lock:
            try:
                self.model = None  # type: ignore[assignment]
            except Exception:
                pass
            try:
                self.processor = None  # type: ignore[assignment]
            except Exception:
                pass

            torch = getattr(self, "_torch", None)
            if torch is not None:
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                except Exception:
                    pass


class Qwen3VLRerankerProvider(BaseRerankerProvider):
    """Local GPU reranker provider using Qwen3-VL-Reranker-* models (multimodal-capable)."""

    _KEEP_TAIL_TOKENS = 5

    def __init__(self, config: ModelConfig):
        self.config = config
        self.model_name = config.model_name

        self.device = config.extra_params.get("device", "cuda:1")
        dtype_str = config.extra_params.get("dtype", "float16")
        attn_impl = config.extra_params.get("attn_implementation", "sdpa")
        self.batch_size = int(config.extra_params.get("batch_size", 8))

        # Qwen3-VL-Reranker supports up to 32k context, but 1080Ti typically needs
        # a smaller max_length to avoid OOM.
        self.max_length = int(config.extra_params.get("max_length", 8192))
        self.min_image_tokens = int(config.extra_params.get("min_image_tokens", 4))
        self.max_image_tokens = int(config.extra_params.get("max_image_tokens", 1280))
        self.allow_image_urls = bool(config.extra_params.get("allow_image_urls", False))

        self.default_instruction = config.extra_params.get(
            "instruction",
            "Given a search query, retrieve relevant candidates that answer the query.",
        )
        self.system_prompt = config.extra_params.get(
            "system_prompt",
            (
                "Judge whether the Document meets the requirements based on the Query and the Instruct provided. "
                'Note that the answer can only be "yes" or "no".'
            ),
        )

        self._lock = asyncio.Lock()

        model_path = config.extra_params.get("model_path", self.model_name)

        try:
            import torch  # pylint: disable=import-error
            from qwen_vl_utils import process_vision_info  # pylint: disable=import-error
            from transformers import (  # pylint: disable=import-error
                AutoProcessor,
                Qwen3VLForConditionalGeneration,
            )
        except Exception as exc:  # pylint: disable=broad-except
            raise RuntimeError(
                "Qwen3-VL reranker provider requires a working torch + transformers installation. "
                "If you're on GTX 1080 Ti, prefer torch CUDA 11.8 wheels (e.g. torch==2.7.1+cu118) "
                "and transformers>=4.57.0 (Qwen3-VL support), plus qwen-vl-utils."
            ) from exc

        self._torch = torch
        self._process_vision_info = process_vision_info
        self.dtype = _resolve_torch_dtype(
            dtype_str, torch_module=torch, device=self.device
        )

        if self.batch_size <= 0:
            raise ValueError(f"Invalid reranker batch_size: {self.batch_size}")
        if self.max_length <= 0:
            raise ValueError(f"Invalid reranker max_length: {self.max_length}")
        if self.min_image_tokens <= 0 or self.max_image_tokens <= 0:
            raise ValueError(
                f"Invalid image token limits: min_image_tokens={self.min_image_tokens}, "
                f"max_image_tokens={self.max_image_tokens}"
            )
        if self.min_image_tokens > self.max_image_tokens:
            raise ValueError(
                f"min_image_tokens ({self.min_image_tokens}) > max_image_tokens ({self.max_image_tokens})"
            )

        logger.info(
            "Loading Qwen3-VL reranker model: %s (path=%s, device=%s, dtype=%s, attn=%s)",
            self.model_name,
            model_path,
            self.device,
            dtype_str,
            attn_impl,
        )

        try:
            lm = Qwen3VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=self.dtype,
                attn_implementation=attn_impl,
                trust_remote_code=True,
            )
            lm = lm.to(self.device)
            lm.eval()

            self.model = lm.model

            self.processor = AutoProcessor.from_pretrained(
                model_path,
                trust_remote_code=True,
            )
            tokenizer = getattr(self.processor, "tokenizer", None)
            if tokenizer is None:
                raise RuntimeError(
                    "Qwen3-VL reranker AutoProcessor did not provide a tokenizer"
                )

            # Qwen3-VL reranker scoring uses the last token hidden state. We use left padding
            # so "[-1]" corresponds to the last *real* token for all rows.
            tokenizer.padding_side = "left"
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            if tokenizer.pad_token_id is None:
                tokenizer.pad_token_id = tokenizer.eos_token_id

            # Build a tiny scoring head from lm_head weights: sigmoid((W_yes - W_no) Â· h_last)
            token_yes_id = self._require_token_id(tokenizer, "yes")
            token_no_id = self._require_token_id(tokenizer, "no")

            self.score_linear = self._build_binary_linear(
                torch_module=torch,
                lm=lm,
                token_yes_id=token_yes_id,
                token_no_id=token_no_id,
            )
            self.score_linear.eval()
            self.score_linear.to(self.device).to(self.model.dtype)

            self._special_ids = set(getattr(tokenizer, "all_special_ids", []))

            logger.info("Performing Qwen3-VL reranker warmup inference...")
            _ = self._rerank_sync(
                query="warmup query",
                documents=["warmup document"],
                instruction=self.default_instruction,
            )

            if (
                torch.cuda.is_available()
                and isinstance(self.device, str)
                and self.device.startswith("cuda")
            ):
                allocated = torch.cuda.memory_allocated(self.device) / 1024**3
                logger.info(
                    "GPU memory allocated after reranker warmup: %.2f GB", allocated
                )

            logger.info("Qwen3-VL reranker model ready")
        except Exception as exc:
            logger.error(
                "Failed to load Qwen3-VL reranker model: %s", exc, exc_info=True
            )
            raise RuntimeError(
                f"Failed to load Qwen3-VL reranker model: {exc}"
            ) from exc

    async def shutdown(self) -> None:
        """Best-effort release of GPU/CPU resources."""
        async with self._lock:
            for attr in ("model", "processor", "score_linear"):
                try:
                    setattr(self, attr, None)
                except Exception:
                    pass

            torch = getattr(self, "_torch", None)
            if torch is not None:
                try:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                except Exception:
                    pass

    @staticmethod
    def _require_token_id(tokenizer: Any, token: str) -> int:
        # Prefer vocab lookup, but fall back to encoding for tokenizers without get_vocab().
        vocab = getattr(tokenizer, "get_vocab", None)
        if callable(vocab):
            token_id = tokenizer.get_vocab().get(token)
            if token_id is not None:
                return int(token_id)

        ids = tokenizer(token, add_special_tokens=False).input_ids
        if len(ids) != 1:
            raise ValueError(
                f'Expected "{token}" to be a single token, got token ids: {ids}'
            )
        return int(ids[0])

    @staticmethod
    def _build_binary_linear(
        *,
        torch_module: Any,
        lm: Any,
        token_yes_id: int,
        token_no_id: int,
    ) -> Any:
        lm_head_weights = lm.lm_head.weight.data
        weight_yes = lm_head_weights[token_yes_id]
        weight_no = lm_head_weights[token_no_id]
        dim = int(weight_yes.size()[0])
        linear = torch_module.nn.Linear(dim, 1, bias=False)
        with torch_module.no_grad():
            linear.weight[0] = weight_yes - weight_no
        return linear

    @staticmethod
    def _truncate_token_ids_preserve_special(
        token_ids: List[int],
        max_length: int,
        special_token_ids: set[int],
    ) -> List[int]:
        if len(token_ids) <= max_length:
            return token_ids

        num_special = sum(1 for token_id in token_ids if token_id in special_token_ids)
        budget_non_special = max(0, max_length - num_special)

        final_ids: List[int] = []
        non_special_kept = 0
        for token_id in token_ids:
            if token_id in special_token_ids:
                final_ids.append(token_id)
                continue
            if non_special_kept < budget_non_special:
                final_ids.append(token_id)
                non_special_kept += 1

        return final_ids

    def _format_pair(
        self, *, instruction: str, query: str, document: str
    ) -> List[Dict[str, Any]]:
        """Format (query, document) pair in the official instruction-aware template.

        The backend rerank contract is text-only, so we infer multimodal inputs by parsing
        ``Image Path: ...`` lines from the query/document text produced by RAGAnything.
        """
        min_pixels = _image_tokens_to_pixels(self.min_image_tokens)
        max_pixels = _image_tokens_to_pixels(self.max_image_tokens)

        query_image = _extract_image_path(query or "")
        doc_image = _extract_image_path(document or "")

        def _allow_image(path: Optional[str]) -> Optional[str]:
            if not path:
                return None
            if path.startswith(("http://", "https://")) and not self.allow_image_urls:
                return None
            if path.startswith("file://"):
                return path if os.path.exists(path[7:]) else None
            if os.path.exists(path):
                return path
            if path.startswith("~"):
                expanded = os.path.expanduser(path)
                return expanded if os.path.exists(expanded) else None
            return None

        query_image = _allow_image(query_image)
        doc_image = _allow_image(doc_image)

        query_text = _strip_image_path_line(query or "").strip() or "NULL"
        doc_text = _strip_image_path_line(document or "").strip() or "NULL"

        user_content: List[Dict[str, Any]] = [
            {"type": "text", "text": "<Instruct>: " + instruction}
        ]

        # Query block
        user_content.append({"type": "text", "text": "<Query>:"})
        if query_image:
            user_content.append(
                {
                    "type": "image",
                    "image": _maybe_prefix_file_scheme(query_image),
                    "min_pixels": min_pixels,
                    "max_pixels": max_pixels,
                }
            )
        user_content.append({"type": "text", "text": query_text})

        # Document block
        user_content.append({"type": "text", "text": "\n<Document>:"})
        if doc_image:
            user_content.append(
                {
                    "type": "image",
                    "image": _maybe_prefix_file_scheme(doc_image),
                    "min_pixels": min_pixels,
                    "max_pixels": max_pixels,
                }
            )
        user_content.append({"type": "text", "text": doc_text})

        return [
            {
                "role": "system",
                "content": [{"type": "text", "text": self.system_prompt}],
            },
            {"role": "user", "content": user_content},
        ]

    def _tokenize_pairs(self, pairs: List[List[Dict[str, Any]]]) -> Dict[str, Any]:
        torch = self._torch

        text = self.processor.apply_chat_template(
            pairs, tokenize=False, add_generation_prompt=True
        )

        try:
            images, video_inputs, video_kwargs = self._process_vision_info(
                pairs,
                image_patch_size=_IMAGE_BASE_FACTOR,
                return_video_kwargs=True,
                return_video_metadata=True,
            )
        except Exception as exc:  # pylint: disable=broad-except
            logger.warning(
                "process_vision_info failed in reranker; falling back to text-only: %s",
                exc,
            )
            images, video_inputs, video_kwargs = None, None, {"do_sample_frames": False}

        if video_inputs is not None:
            videos, video_metadatas = zip(*video_inputs)
            videos, video_metadatas = list(videos), list(video_metadatas)
        else:
            videos, video_metadatas = None, None

        inputs = self.processor(
            text=text,
            images=images,
            videos=videos,
            video_metadata=video_metadatas,
            truncation=False,
            padding=False,
            do_resize=False,
            **(video_kwargs or {}),
        )

        input_ids = inputs.get("input_ids")
        if not isinstance(input_ids, list):
            raise RuntimeError(
                "Processor returned unexpected input_ids type for reranker"
            )

        special_ids = self._special_ids
        keep_tail = self._KEEP_TAIL_TOKENS
        budget = max(0, self.max_length - keep_tail)

        truncated: List[List[int]] = []
        for ids in input_ids:
            tail = ids[-keep_tail:] if keep_tail > 0 else []
            head = ids[:-keep_tail] if keep_tail > 0 else ids
            head = self._truncate_token_ids_preserve_special(head, budget, special_ids)
            truncated.append(head + tail)

        padded = self.processor.tokenizer.pad(
            {"input_ids": truncated},
            padding=True,
            return_tensors="pt",
            max_length=self.max_length,
        )

        # Merge padded input_ids/attention_mask back into the processor outputs (which may already
        # contain image/video tensors), then move tensors to device.
        for key, value in padded.items():
            inputs[key] = value

        # Prefer processor's .to() (BatchFeature/BatchEncoding) when available.
        to_fn = getattr(inputs, "to", None)
        if callable(to_fn):
            moved = to_fn(self.device)
            return {k: v for k, v in moved.items() if isinstance(v, torch.Tensor)}

        return {
            k: v.to(self.device)
            for k, v in inputs.items()
            if isinstance(v, torch.Tensor)
        }

    async def rerank(self, query: str, documents: List[str], **kwargs) -> List[float]:
        if not documents:
            return []

        instruction = kwargs.get("instruction", self.default_instruction)
        loop = asyncio.get_event_loop()

        async with self._lock:
            return await loop.run_in_executor(
                None, self._rerank_sync, query, list(documents), instruction
            )

    def _rerank_sync(
        self, query: str, documents: List[str], instruction: str
    ) -> List[float]:
        torch = self._torch

        scores: List[float] = []
        with torch.no_grad():
            for start in range(0, len(documents), self.batch_size):
                batch_docs = documents[start : start + self.batch_size]
                pairs = [
                    self._format_pair(
                        instruction=instruction, query=query, document=doc
                    )
                    for doc in batch_docs
                ]

                inputs = self._tokenize_pairs(pairs)
                outputs = self.model(**inputs)
                hidden_last = _pool_last_token(
                    outputs.last_hidden_state,
                    inputs.get("attention_mask"),
                    torch_module=torch,
                )
                batch_scores = torch.sigmoid(self.score_linear(hidden_last)).squeeze(-1)
                scores.extend(batch_scores.float().cpu().tolist())

        return scores
