# RAG-Anything v2.0 - New Architecture

> **Custom Frontend/Backend Architecture** replacing OpenWebUI integration

> âš ï¸ Note: This file is kept as a historical design note. The current, maintained
> startup/config instructions are in `README.md` (React/Vite frontend + FastAPI backend).

## ğŸ¯ What's New

- âœ… **Custom React Frontend** - Clean, simple UI for document upload and queries
- âœ… **FastAPI Backend** - Production-ready REST API
- âœ… **Flexible Model Providers** - Support for OpenAI, Azure, LM Studio, vLLM, and any OpenAI-compatible API
- âœ… **No Ollama Dependency** - Use any LLM provider via base_url + api_key
- âœ… **Unified Configuration** - Environment variables or YAML files
- âœ… **Type-Safe** - Pydantic validation throughout
- âœ… **Async-Native** - High performance with async/await

## ğŸš€ Quick Start (5 Minutes)

### 1. Install Dependencies

```bash
# Backend (recommended)
uv sync

# Frontend
cd frontend
npm install
cd ..
```

### 2. Configure Environment

**Unified Configuration (Recommended):**
```bash
cp env.example .env
```

Edit `.env` with your API keys:

```bash
# Minimal OpenAI configuration
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o-mini
LLM_API_KEY=sk-your-api-key-here

EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL_NAME=text-embedding-3-large
EMBEDDING_API_KEY=sk-your-api-key-here
EMBEDDING_EMBEDDING_DIM=3072
```

**Frontend Configuration (Optional):** see `frontend/env.example` (only `VITE_` vars are exposed to the browser)

> Tip: dev æ¨¡å¼ä¸‹å‰ç«¯é»˜è®¤èµ° Vite proxyï¼Œä¸€èˆ¬æ— éœ€é…ç½® `VITE_BACKEND_URL`ã€‚

### 3. Start Services

```bash
# Start both backend and frontend
bash scripts/start_all.sh
```

### 4. Open Browser

- **Frontend**: http://localhost:5173
- **API Docs**: http://localhost:8000/docs

## ğŸ“ Project Structure

```
RAG-Anything/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ config.py        # Unified configuration
â”‚   â”œâ”€â”€ main.py          # API server
â”‚   â”œâ”€â”€ models/          # Pydantic models
â”‚   â”œâ”€â”€ providers/       # Model provider adapters
â”‚   â”œâ”€â”€ routers/         # API endpoints
â”‚   â””â”€â”€ services/        # Business logic
â”œâ”€â”€ frontend/            # React + Vite UI
â”œâ”€â”€ configs/             # Configuration examples
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ scripts/             # Deployment scripts
```

## ğŸ”§ Supported Providers

### OpenAI

```bash
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o-mini
LLM_API_KEY=sk-...
```

### Azure OpenAI

```bash
LLM_PROVIDER=azure
LLM_MODEL_NAME=gpt-4o-mini
LLM_API_KEY=your-azure-key
LLM_BASE_URL=https://your-resource.openai.azure.com/
```

### LM Studio (Local)

```bash
LLM_PROVIDER=local
LLM_MODEL_NAME=local-model
LLM_BASE_URL=http://localhost:1234/v1
```

### Custom API (vLLM, TGI, etc.)

```bash
LLM_PROVIDER=custom
LLM_MODEL_NAME=Qwen/Qwen2.5-72B-Instruct
LLM_API_KEY=your-key
LLM_BASE_URL=https://your-endpoint.com/v1
```

## ğŸ“š Documentation

- **[Quick Start Guide](docs/QUICKSTART_NEW_ARCHITECTURE.md)** - Get started in 5 minutes
- **[Architecture Details](docs/ARCHITECTURE_REDESIGN.md)** - Detailed architecture documentation
- **[Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md)** - What was built and why

## ğŸ¨ Features

### Frontend (Gradio)

- ğŸ“„ **Document Upload** - Upload PDF, DOCX, PPTX, XLSX, TXT, MD, HTML
- ğŸ’¬ **Chat Interface** - Ask questions with conversation history
- âš™ï¸ **Configuration Panel** - View backend configuration
- ğŸ” **Query Modes** - Naive, Local, Global, Hybrid

### Backend (FastAPI)

- ğŸ“¤ **Document Management** - Upload, process, batch processing
- ğŸ” **Query API** - Standard, multimodal, conversational queries
- ğŸ¥ **Health Checks** - Status and readiness endpoints
- ğŸ“– **Auto-Generated Docs** - OpenAPI/Swagger at `/docs`

## ğŸ”Œ API Endpoints

### Documents

- `POST /api/documents/upload` - Upload document
- `POST /api/documents/process` - Process document
- `POST /api/documents/upload-and-process` - Upload and process
- `POST /api/documents/batch-process` - Batch process folder
- `GET /api/documents/list` - List documents

### Query

- `POST /api/query/` - Execute RAG query
- `POST /api/query/multimodal` - Multimodal query
- `POST /api/query/conversation` - Conversational query

### Health

- `GET /health` - Health check
- `GET /ready` - Readiness check

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React UI       â”‚  Frontend (Port 5173)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/REST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI        â”‚  Backend (Port 8000)
â”‚  - Routers      â”‚
â”‚  - Services     â”‚
â”‚  - Providers    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAGAnything    â”‚  Core Library
â”‚  - LightRAG     â”‚
â”‚  - MineRU       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM APIs       â”‚  External Services
â”‚  - OpenAI       â”‚
â”‚  - Azure        â”‚
â”‚  - Custom       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Migration from Old Architecture

### What Changed

- âŒ Removed OpenWebUI integration
- âŒ Removed Ollama-specific code
- âœ… Added FastAPI backend
- âœ… Added React frontend (migrated from Gradio)
- âœ… Added unified configuration

### What Stayed the Same

- âœ… Core RAGAnything library
- âœ… Document processing (MineRU/Docling)
- âœ… LightRAG integration
- âœ… Query modes (Naive/Local/Global/Hybrid)

## ğŸ› ï¸ Development

### Start Backend Only

```bash
bash scripts/start_backend.sh
```

### Start Frontend Only

```bash
bash scripts/start_frontend.sh
```

### Enable Auto-Reload (Development)

```bash
python -m backend.main --reload
```

## ğŸ§ª Testing

### Test Backend Health

```bash
curl http://localhost:8000/health
```

### Test Document Upload

```bash
curl -X POST http://localhost:8000/api/documents/upload-and-process \
  -F "file=@document.pdf"
```

### Test Query

```bash
curl -X POST http://localhost:8000/api/query/ \
  -H "Content-Type: application/json" \
  -d '{"query": "What is this about?", "mode": "hybrid"}'
```

## ğŸ“ Configuration Examples

See `configs/model_providers.yaml` for complete examples:

- OpenAI configuration
- Azure OpenAI configuration
- LM Studio (local) configuration
- Mixed providers configuration
- Custom API configuration

## ğŸ¤ Contributing

The new architecture is designed to be extensible:

1. **Add new providers**: Implement `BaseLLMProvider` in `backend/providers/`
2. **Add new endpoints**: Create routers in `backend/routers/`
3. **Customize frontend**: Edit `frontend/app.py`

## ğŸ“„ License

Same as RAG-Anything main project.

## ğŸ™ Acknowledgments

- **RAGAnything** - Core multimodal RAG library
- **LightRAG** - Knowledge graph RAG framework
- **FastAPI** - Modern async web framework
- **Gradio** - ML/AI web interface framework

---

**Ready to get started?** See [Quick Start Guide](docs/QUICKSTART_NEW_ARCHITECTURE.md)
