# Arona unified configuration (recommended)
# =======================================
#
# Quick start:
#   cp env.example .env
#   bash scripts/start_all.sh
#
# Notes:
# - Backend reads `.env` (preferred) or `.env.backend` (legacy).
# - Frontend only exposes variables prefixed with `VITE_` to the browser.
#   Do NOT prefix secrets with `VITE_`.
#

# -----------------------------------------------------------------------------
# Backend: required LLM + Embedding configuration
# -----------------------------------------------------------------------------
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o-mini
LLM_API_KEY=sk-your-api-key-here

# Default (local GPU): Qwen3-VL-Embedding-2B
# Notes:
# - Requires a working PyTorch+CUDA runtime and transformers>=4.57.0 (see requirements-backend.txt)
# - If you prefer an API-based embedding model, switch EMBEDDING_PROVIDER back to openai/azure/custom.
EMBEDDING_PROVIDER=local_gpu
EMBEDDING_MODEL_NAME=Qwen/Qwen3-VL-Embedding-2B
EMBEDDING_DEVICE=cuda:0
EMBEDDING_DTYPE=float16
EMBEDDING_ATTN_IMPLEMENTATION=sdpa
EMBEDDING_MAX_LENGTH=8192
EMBEDDING_EMBEDDING_DIM=2048
EMBEDDING_DEFAULT_INSTRUCTION="Represent the user's input."
EMBEDDING_MIN_IMAGE_TOKENS=4
EMBEDDING_MAX_IMAGE_TOKENS=1280
EMBEDDING_ALLOW_IMAGE_URLS=false
EMBEDDING_NORMALIZE=true

# Default (local GPU): Qwen3-VL-Reranker-2B
# Tip: If you have 2 GPUs, set RERANKER_DEVICE=cuda:1 to reduce contention.
RERANKER_ENABLED=true
RERANKER_PROVIDER=local_gpu
RERANKER_MODEL_NAME=Qwen/Qwen3-VL-Reranker-2B
RERANKER_DEVICE=cuda:0
RERANKER_DTYPE=float16
RERANKER_ATTN_IMPLEMENTATION=sdpa
RERANKER_MAX_LENGTH=8192
RERANKER_BATCH_SIZE=8
RERANKER_MIN_IMAGE_TOKENS=4
RERANKER_MAX_IMAGE_TOKENS=1280
RERANKER_ALLOW_IMAGE_URLS=false

# -----------------------------------------------------------------------------
# Backend: optional server settings
# -----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000

# -----------------------------------------------------------------------------
# Backend: optional storage settings
# -----------------------------------------------------------------------------
WORKING_DIR=./rag_storage
UPLOAD_DIR=./uploads

# -----------------------------------------------------------------------------
# Backend: Chat settings
# -----------------------------------------------------------------------------
# Auto-attach retrieved images: If enabled, images found in the RAG retrieval
# context will be automatically appended to assistant responses.
CHAT_AUTO_ATTACH_RETRIEVED_IMAGES=true
# Maximum number of retrieved images to attach per response (0 to disable)
CHAT_MAX_RETRIEVED_IMAGES=4

# -----------------------------------------------------------------------------
# Frontend: optional
# -----------------------------------------------------------------------------
# In dev mode, the frontend uses Vite proxy (/api -> backend) by default.
# For production builds (or when frontend/backend are on different origins),
# set VITE_BACKEND_URL explicitly.
# VITE_BACKEND_URL=http://localhost:8000

# Override frontend dev/preview host/port used by scripts (optional)
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=5173

# -----------------------------------------------------------------------------
# MinerU / HuggingFace cache (optional)
# -----------------------------------------------------------------------------
# If you rely on MinerU and want deterministic caches, set these.
# Tip: use absolute paths when running backend directly without shell scripts.
# HF_HOME=/absolute/path/to/.huggingface
# HF_HUB_CACHE=/absolute/path/to/.huggingface/hub
# MINERU_MODEL_SOURCE=huggingface   # huggingface | modelscope | local

# -----------------------------------------------------------------------------
# MinerU runtime (optional)
# -----------------------------------------------------------------------------
# Tip: If you have multiple GPUs, run MinerU on a less busy GPU to avoid CUDA OOM.
# MINERU_DEVICE=cuda:1
# Optional: Upper limit of GPU memory (MiB) for a single MinerU process (pipeline backend)
# MINERU_VRAM=9000
# Optional: Reduce PyTorch CUDA allocator fragmentation (can help near-OOM cases)
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
